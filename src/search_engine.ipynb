{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf89179",
   "metadata": {},
   "source": [
    "### Setup Elasticsearch cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bca8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "get_ipython().system = os.system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b72876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Elasticsearch binaries into downloads folder\n",
    "# <YOUR PASSWORD> is your sudo password\n",
    "!mkdir ../downloads\n",
    "!wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz -P ../downloads\n",
    "!wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 -P ../downloads\n",
    "!tar -xzf ../downloads/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz -C ../downloads\n",
    "!echo \"<YOUR PASSWORD>\" | sudo chown -R daemon:daemon ../downloads/elasticsearch-7.9.2/\n",
    "!shasum -a 512 -c ../downloads/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Elasticsearch server as bg process\n",
    "!echo \"<YOUR PASSWORD>\" | sudo -HSu daemon ../downloads/elasticsearch-7.9.2/bin/elasticsearch &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01c122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the daemon process status\n",
    "!!ps -ef | grep elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 9200\n",
    "host = f'http://localhost:{port}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cluster started up correctly\n",
    "time.sleep(30)\n",
    "!!curl -s {host}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bfd86",
   "metadata": {},
   "source": [
    "## Collect browser history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import os\n",
    "import glob\n",
    "import platform\n",
    "import shutil\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c897bbb",
   "metadata": {},
   "source": [
    "#### Base aggregator\n",
    "Every browser implements the base class as their internal database schemas are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c998f6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    def __init__(self):\n",
    "        self.browsing_history = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        # scan browsers internal history dbs\n",
    "        curr_os = platform.system()\n",
    "        home_dir = path.expanduser('~')\n",
    "        oss_bin_paths = {\n",
    "            'Linux'  : ('/', 'usr', 'bin'),\n",
    "            'Darwin' : ('/', 'Applications'),\n",
    "            'Windows': ('C:/', 'Program Files')\n",
    "        }\n",
    "        # /usr/bin: symlinked from /usr/lib\n",
    "        install_dirs = {\n",
    "            os : path.join(*path_comps) \n",
    "            for os, path_comps in oss_bin_paths.items()\n",
    "        }\n",
    "        browsers_data_stores = {\n",
    "            'Chrome' : ('.config', 'google-chrome', 'Default', 'History'),\n",
    "            'Firefox' : ('.mozilla', 'firefox', '*.default*', 'places.sqlite')\n",
    "        }\n",
    "        browsers = {\n",
    "            browser : path.join(home_dir, *path_comps) \n",
    "            for browser, path_comps in browsers_data_stores.items()\n",
    "        }        \n",
    "        # store both locked and lock-free db copies\n",
    "        self.db_files = {\n",
    "            browser : {\n",
    "                file_t : {} for file_t in ['orig', 'tmp']\n",
    "            }\n",
    "            for browser in browsers\n",
    "        }\n",
    "        \n",
    "        for browser in browsers:\n",
    "            found = glob.glob(f'{install_dirs[curr_os]}/*{browser.lower()}*') is not None\n",
    "            if found:\n",
    "                orig_file = glob.glob(browsers[browser])\n",
    "                tmp_file = self._tmp_copy(*orig_file)\n",
    "                self.db_files[browser]['orig'] = orig_file\n",
    "                self.db_files[browser]['tmp'] = tmp_file\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # delete tmp copies on exit\n",
    "        for _, files in self.db_files.items():\n",
    "            os.remove(files['tmp'])\n",
    "                    \n",
    "    def _tmp_copy(self, original_file):\n",
    "        tmp = tempfile.gettempdir()\n",
    "        filename = path.basename(original_file)\n",
    "        tmp_file = path.join(tmp, filename)\n",
    "        shutil.copy2(original_file, tmp_file)\n",
    "        return tmp_file\n",
    "        \n",
    "    @classmethod\n",
    "    def _regexp(cls, regex, field_val):\n",
    "        # base REGEXP implementation\n",
    "        return bool(re.search(regex, field_val))\n",
    "    \n",
    "    def _extract(self, row):\n",
    "        return row[0]\n",
    "    \n",
    "    def _get_history_tables(self, conn, cursor):\n",
    "        conn.create_function(\"REGEXP\", 2, Aggregator._regexp)\n",
    "        regex = '.*(history|visit).*'\n",
    "        query = cursor.execute(\"\"\"\n",
    "            SELECT name FROM sqlite_master \n",
    "            WHERE type=\"table\" and name REGEXP ?\n",
    "        \"\"\", [regex])\n",
    "        rows = query.fetchall()\n",
    "        rows = [*map(self._extract, rows)]\n",
    "        return rows\n",
    "   \n",
    "    def _get_fields(self, table):\n",
    "        # looksup a table's columns\n",
    "        query = self.cursor.execute(f'SELECT * FROM {table}')\n",
    "        fields = [*map(self._extract, query.description)]\n",
    "        return fields\n",
    "            \n",
    "    def _to_dict(self, row, fields):\n",
    "        # converts row to dict by mapping columns to values\n",
    "        dict_ = {}\n",
    "        for i, val in enumerate(row):\n",
    "            field = self._extract(fields[i])\n",
    "            if field == 'visit_type':\n",
    "                val = self._visit_type(val)\n",
    "                val = val.split('_')[-1].lower()\n",
    "            dict_[field] = val\n",
    "        return dict_\n",
    "    \n",
    "    def _to_json(self, cursor):\n",
    "        fields = cursor.description\n",
    "        rows = cursor.fetchall()\n",
    "        rows = [ self._to_dict(row, fields) for row in rows ]\n",
    "        # rows_as_json = json.dumps(rows, indent = 2)\n",
    "        return rows\n",
    "    \n",
    "    def merge(self, history):\n",
    "        if not self.browsing_history:\n",
    "            self.browsing_history = history\n",
    "        else:\n",
    "            self.browsing_history += history\n",
    "                                 \n",
    "    def get_history_as_json(self):\n",
    "        # returns browser history as list of json documents\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def save(self):\n",
    "        if self.browsing_history:\n",
    "            file_path = os.path.join('../dataset', 'browsing_history.json')\n",
    "            with open(file_path, 'w') as file:\n",
    "                json.dump(self.browsing_history, file, indent = 2)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2ebfe",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AggregatorFirefox(Aggregator):\n",
    "    def __init__(self, db_file):\n",
    "        # raises sqlite3.OperationalError: unable to open database file\n",
    "        self.conn = sqlite3.connect(f'file:{db_file}?mode=ro', uri = True)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        sqlite3.enable_callback_tracebacks(True)\n",
    "            \n",
    "    def _visit_type(self, enum):\n",
    "        # maps visit types to their written rep for ease of querying\n",
    "        return [\n",
    "            'TRANSITION_LINK',\n",
    "            'TRANSITION_TYPED',\n",
    "            'TRANSITION_BOOKMARK',\n",
    "            'TRANSITION_EMBED',\n",
    "            'TRANSITION_REDIRECT_PERMANENT',\n",
    "            'TRANSITION_REDIRECT_TEMPORARY',\n",
    "            'TRANSITION_DOWNLOAD',\n",
    "            'TRANSITION_FRAMED_LINK',\n",
    "            'TRANSITION_RELOAD'\n",
    "        ][enum - 1]\n",
    "\n",
    "    def get_history_as_json(self):\n",
    "        # returns browser history as list of json documents\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT moz_historyvisits.id,\n",
    "                   moz_places.url, \n",
    "                   moz_places.title, \n",
    "                   moz_places.visit_count,\n",
    "                   (\n",
    "                       SELECT printf(\"%d\", total(use_count))\n",
    "                       FROM moz_inputhistory \n",
    "                       WHERE moz_inputhistory.place_id = moz_places.id\n",
    "                   ) typed_count,\n",
    "                   DATETIME(moz_places.last_visit_date/1000000,'unixepoch') as last_visit_date, \n",
    "                   DATETIME(moz_historyvisits.visit_date/1000000, 'unixepoch') as visit_date, \n",
    "                   moz_historyvisits.from_visit,\n",
    "                   moz_historyvisits.visit_type,\n",
    "                   'Firefox' as browser\n",
    "            FROM moz_places, moz_historyvisits \n",
    "            WHERE moz_historyvisits.place_id = moz_places.id;\n",
    "        \"\"\")\n",
    "        rows_as_json = self._to_json(self.cursor)\n",
    "        return rows_as_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744d7e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AggregatorChrome(Aggregator):\n",
    "    def __init__(self, db_file):\n",
    "        # raises sqlite3.OperationalError: unable to open database file\n",
    "        self.conn = sqlite3.connect(f'file:{db_file}?mode=ro', uri = True)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        sqlite3.enable_callback_tracebacks(True)\n",
    "         \n",
    "    def _visit_type(self, enum):\n",
    "        # maps visit types to their written rep for ease of querying\n",
    "        CORE_MASK = 0xFF\n",
    "        return [\n",
    "            'LINK',\n",
    "            'TYPED',\n",
    "            'AUTO_BOOKMARK',\n",
    "            'AUTO_SUBFRAME',\n",
    "            'MANUAL_SUBFRAME',\n",
    "            'GENERATED',\n",
    "            'START_PAGE',\n",
    "            'FORM_SUBMIT',\n",
    "            'RELOAD',\n",
    "            'KEYWORD',\n",
    "            'KEYWORD_GENERATED'\n",
    "        ][enum & CORE_MASK]\n",
    "\n",
    "    def get_history_as_json(self):\n",
    "        # returns browser history as list of json documents\n",
    "        self.cursor.execute(\"\"\"\n",
    "            SELECT urls.id,\n",
    "                   urls.url, \n",
    "                   urls.title, \n",
    "                   urls.visit_count, \n",
    "                   urls.typed_count, \n",
    "                   DATETIME(urls.last_visit_time / 1000000 + (strftime('%s', '1601-01-01')), 'unixepoch', 'localtime') as last_visit_date, \n",
    "                   DATETIME(visit_time / 1000000 + (strftime('%s', '1601-01-01')), 'unixepoch', 'localtime') as visit_date, \n",
    "                   visits.from_visit, \n",
    "                   visits.transition as visit_type,\n",
    "                   'Chrome' as browser\n",
    "            FROM urls, visits\n",
    "            WHERE urls.id = visits.url\n",
    "        \"\"\")\n",
    "        rows_as_json = self._to_json(self.cursor)\n",
    "        return rows_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad2629",
   "metadata": {},
   "source": [
    "#### Extract browsers' history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb665973",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Aggregator() as agg:\n",
    "    db_files = agg.db_files\n",
    "    for browser, AggregatorBrowser in list(zip(db_files, [AggregatorChrome, AggregatorFirefox])):\n",
    "        db_file = db_files[browser]['tmp']\n",
    "        print(f'\\n{browser}: ', db_file, end=\"\\n\\n\")\n",
    "        try:\n",
    "            agg_browser = AggregatorBrowser(db_file) \n",
    "            tables = agg_browser._get_history_tables(agg_browser.conn, agg_browser.cursor)\n",
    "\n",
    "            for table in tables:\n",
    "                fields = agg_browser._get_fields(table)\n",
    "                print(table, fields, sep = \"\\n\", end = \"\\n\\n\")\n",
    "\n",
    "            json_docs = agg_browser.get_history_as_json()\n",
    "            # print(json_docs)\n",
    "            agg.merge(json_docs)\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(str(e))\n",
    "    \n",
    "    print(json.dumps(agg.browsing_history, indent = 2))\n",
    "    \n",
    "    agg.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17610c3a",
   "metadata": {},
   "source": [
    "## Build a search engine\n",
    "Make sure to install `elasticsearch==7.9.1` (if not installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5745fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install elasticsearch==7.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch as es\n",
    "import elasticsearch.helpers as helpers\n",
    "from uuid import uuid4\n",
    "from datetime import datetime as dt\n",
    "import json\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86940351",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    def __init__(self, host):\n",
    "        self.client = es.Elasticsearch(host)\n",
    "        self.docs = self.read_browser_history()\n",
    "        ping = self.client.ping()\n",
    "        if not ping:\n",
    "            raise Exception('Error: could not connect to cluster')\n",
    "        print('Ok: cluster is up')\n",
    "    \n",
    "    def read_browser_history(self):\n",
    "        docs_path = path.join('..', 'dataset', 'browsing_history.json')\n",
    "        with open(docs_path, 'r') as f:\n",
    "            docs = json.load(f)\n",
    "            return docs\n",
    "\n",
    "    def cluster_info(self):\n",
    "        print(json.dumps(self.client.info(), indent = 2))\n",
    "    \n",
    "    def create_index(self, index_name, _doc, sim_module):\n",
    "        # get document sample\n",
    "        sample_doc = self.docs[0]\n",
    "        # extract mappings from document sample\n",
    "        mappings = self._extract_mappings(sample_doc)\n",
    "        request_body = {\n",
    "            'settings': {\n",
    "                'number_of_shards': 1,\n",
    "                'number_of_replicas': 1,\n",
    "                'similarity' : sim_module\n",
    "            }\n",
    "        }\n",
    "        request_body.update(mappings)\n",
    "        # print(request_body)\n",
    "        if self.client.indices.exists(index_name):\n",
    "            raise Exception(f'Error: index {index_name} exists.')\n",
    "        self.client.indices.create(index_name, body = request_body, ignore = 400)\n",
    "        # bulk index docs\n",
    "        self._do_index(self.docs, index_name, _doc)        \n",
    "        print(f'OK: index {index_name} created.')\n",
    "        \n",
    "    def _do_index(self, docs, _index, _doc):\n",
    "        def bulk(docs, _index, _doc):\n",
    "            for i, doc in enumerate(docs):\n",
    "                action = {\n",
    "                    \"_index\": _index,\n",
    "                    \"_doc\": _doc,\n",
    "                    \"_id\": i,\n",
    "                    \"_source\": {\n",
    "                        key : value \n",
    "                        for key, value in doc.items()\n",
    "                    }\n",
    "                }\n",
    "                yield action\n",
    "        \n",
    "        try:\n",
    "            res = helpers.bulk(self.client, bulk(docs, _index, _doc))\n",
    "            succ, fail = res\n",
    "            print(f'Ok: success: {succ}; fail: {fail}')\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    \n",
    "    def update_ranking_model(self, index_name, sim_module):\n",
    "        name = [*sim_module.keys()][0]\n",
    "        settings = { \n",
    "            'settings' : {\n",
    "                'index' : {\n",
    "                    'similarity' : sim_module\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.client.indices.close(index = index_name)\n",
    "        self.client.indices.put_settings(index = index_name, body = settings)\n",
    "        self.client.indices.open(index = index_name)\n",
    "        \n",
    "        model_type = sim_module[name]['type'].lower()\n",
    "        base_index_name = index_name.split('_')[-1]\n",
    "        new_index_name = f'{name}_{base_index_name}'\n",
    "                \n",
    "        if self._re_index(index_name, new_index_name):\n",
    "            # delete old index\n",
    "            self.client.indices.delete(index = index_name)\n",
    "            if self._update_alias(new_index_name, base_index_name):                \n",
    "                print(f\"Index {index_name} updated with ranking model {model_type}\")\n",
    "        else:\n",
    "            print(f\"Failed to update {index_name} with ranking model {model_type}\")\n",
    "    \n",
    "    def _re_index(self, index_name, new_index_name):\n",
    "        # reindexes the old index with a new name\n",
    "        res = self.client.reindex({\n",
    "            'source' : {\n",
    "                'index' : index_name\n",
    "            },\n",
    "            'dest' : {\n",
    "                'index' : new_index_name\n",
    "            }\n",
    "        })['total'] > 0\n",
    "        return res\n",
    "    \n",
    "    def _update_alias(self, index_name, alias):\n",
    "        # creates alias with old index name to new index name\n",
    "        # so we can keep using the old index name\n",
    "        # e.g., history -> dfr_history\n",
    "        res = bool(self.client.indices.update_aliases(body = {\n",
    "            'actions' : [{\n",
    "                'add'  : {\n",
    "                    'index' : index_name,\n",
    "                    'alias' : alias\n",
    "                }\n",
    "            }]\n",
    "        })['acknowledged'])\n",
    "        return res\n",
    "    \n",
    "    def index_info(self, index_name = None):\n",
    "        if index_name:\n",
    "            info = json.dumps(self.client.cat.indices(format = 'json', index = index_name), indent = 2)\n",
    "        else:\n",
    "            info = json.dumps(self.client.cat.indices(format = 'json'), indent = 2)\n",
    "        print(info)\n",
    "            \n",
    "    def _convert_to_date(self, field):\n",
    "        try:\n",
    "            date = dt.strptime(field, '%Y-%m-%d %H:%M:%S')\n",
    "            return date\n",
    "        except:\n",
    "            return field\n",
    "            \n",
    "    def _extract_mappings(self, sample):\n",
    "        sample_ = sample.copy()\n",
    "        sanitised_vals = [*map(self._convert_to_date, list(sample_.values()))]\n",
    "        sample_.update(\n",
    "            (field, val) \n",
    "            for field, val in zip(\n",
    "                sample_.keys(), sanitised_vals\n",
    "            )\n",
    "        )   \n",
    "        # print(sample_)\n",
    "        types = {\n",
    "            'int'      : 'integer',\n",
    "            'str'      : 'text',\n",
    "            'datetime' : 'date'\n",
    "        }\n",
    "        return {\n",
    "            'mapping' : {\n",
    "                '_source' : {\n",
    "                    'enabled' : 'true'\n",
    "                },\n",
    "                'properties' : {\n",
    "                    property_ : { \n",
    "                        'type' : types[type(property_val).__name__] \n",
    "                    }\n",
    "                    for property_, property_val in sample_.items()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def query(self, index, body = {\"size\": 10000, \"query\": {\"match_all\": {}}}):\n",
    "        return self.client.search(body = body, index = index, explain = True)\n",
    "    \n",
    "    def get_hits(self, results, *fields, explain = True, fmt = 'json'):\n",
    "        if fmt == 'json':\n",
    "            return {\n",
    "                'count' : results['hits']['total']['value'],\n",
    "                'hits' : {\n",
    "                    hit['_id'] : {\n",
    "                        **({ field : hit['_source'][field] for field in fields}),\n",
    "                        'score': hit['_score'],\n",
    "                        **({ 'explanation' : hit['_explanation'] } if explain else {})\n",
    "                    } \n",
    "                    for hit in results['hits']['hits']\n",
    "                }\n",
    "            }\n",
    "        elif fmt == 'ascii':\n",
    "            fmt_hits = []\n",
    "            for hit in results['hits']['hits']:\n",
    "                fmt_str = f\"id: {hit['_id']}\\n\"\n",
    "                for field in fields:\n",
    "                    fmt_str += f\"{field}: {hit['_source'][field]}\\n\"\n",
    "                fmt_str += f\"score: {hit['_score']}\\n\"\n",
    "                fmt_hits.append(fmt_str)\n",
    "            return '\\n'.join(fmt_hits)\n",
    "        else:\n",
    "            raise ValueError(f'Error: unrecognised format {fmt}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2c769",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiate the search engine\n",
    "try:\n",
    "    se = SearchEngine(host)\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se.cluster_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can manually delete an index or all - * - if you feel you messed somewhere\n",
    "se.client.indices.delete(index = '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a781c",
   "metadata": {},
   "source": [
    "## Ranking models\n",
    "_BM25 similarity (default)_\n",
    "\n",
    "Note: we're required to explain how ranking works for each of the models used, i.e. how it reflects in the documents returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ab3ad",
   "metadata": {},
   "source": [
    "### BM25 (Best Match Okapi)\n",
    "This is the default ranking model used by Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_bm25  = {\n",
    "    'sim_bm25' : {\n",
    "        'type' : 'BM25',\n",
    "        'b' : '0.75',\n",
    "        'k1' : 1.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35cd45",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create index\n",
    "index_name = 'history'\n",
    "_doc = 'browser_history'\n",
    "try:\n",
    "    se.create_index(index_name, _doc, sim_bm25)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78bdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se.index_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac066f2",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all records\n",
    "q = se.query(index_name)\n",
    "print(json.dumps(q, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0104169",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search for `sqlite` in  title\n",
    "query = {\n",
    "    'query' : {\n",
    "        'term'  : {\n",
    "            'title' : 'sqlite'\n",
    "        } \n",
    "    } \n",
    "}\n",
    "\n",
    "\n",
    "res = se.query(index_name, body = query)\n",
    "hits = se.get_hits(res, *['url', 'title', 'visit_date', 'last_visit_date'], explain = False, fmt = 'ascii')\n",
    "# if you want to print as json use `json.dumps(hits, indent = 2)`\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce127d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bool clause allows us to build boolean expressions\n",
    "# `should` behaves like an OR clause whereas `must` behaves like an AND clause\n",
    "# this can be used across many fields\n",
    "\n",
    "# search for either keyword in title\n",
    "keywords = 'sqlite documentation history'\n",
    "query = {\n",
    "    'query' : {\n",
    "        'bool' : {\n",
    "            'should' : [\n",
    "                {\n",
    "                    'terms' : {\n",
    "                        'title' : keywords.split(' ')\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "res = se.query(index_name, body = query)\n",
    "hits = se.get_hits(res, *['url', 'title', 'visit_date', 'last_visit_date'], explain = False)\n",
    "print(json.dumps(hits, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36cb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for documents whose `last_visit_date` is more recent than March 29 and contain\n",
    "# either history on the `title` or google somewhere in the URL\n",
    "# note: /google/ is a regex pattern.\n",
    "query = {\n",
    "    'query' : {\n",
    "        'bool' : {\n",
    "            'must' : [\n",
    "                {\n",
    "                    'range' : {\n",
    "                        'last_visit_date' : {\n",
    "                            'gte' : '2023-29-03'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            'should' : [\n",
    "                {\n",
    "                    'match'  : { \n",
    "                        'title' : 'history'\n",
    "                    },\n",
    "                    'match' : {\n",
    "                        'url' :  '/google/'\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "res = se.query(index_name, body = query)\n",
    "hits = se.get_hits(res, *['url', 'title', 'visit_date', 'last_visit_date'], explain = False)\n",
    "print(json.dumps(hits, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'query': {\n",
    "        'range': {\n",
    "            'last_visit_date' : {\n",
    "#                 \"gte\": \"2023-01-01 00:00:00\",\n",
    "                \"lte\": \"2023-03-18 00:00:00\",\n",
    "                \"format\": \"yyyy-MM-dd HH-mm-ss\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "res = se.query(index_name, body = query)\n",
    "hits = se.get_hits(res, *['url', 'title', 'visit_date', 'last_visit_date'], explain = False)\n",
    "print(json.dumps(hits, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbb1fd",
   "metadata": {},
   "source": [
    "### DFR (Divergence from Randomness)\n",
    "This model takes into account statistical properties of the collection, e.g. frequency and distribution of terms within the collection, length of documents, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dfr = {\n",
    "    \"sim_dfr\": {\n",
    "      \"type\": \"DFR\",\n",
    "      \"basic_model\": \"g\",\n",
    "      \"after_effect\": \"l\",\n",
    "      \"normalization\": \"h2\",\n",
    "      \"normalization.h2.c\": \"2.0\"\n",
    "    }\n",
    "}\n",
    "se.update_ranking_model(index_name, sim_dfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b722f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "se.index_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ccfd7",
   "metadata": {},
   "source": [
    "### BM25F\n",
    "This can be achieved using a `multi_match` query, which allows us to assign different weights to each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ebca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_bm25f = {\n",
    "    \"sim_bm25f\": {\n",
    "        'type' : 'BM25',\n",
    "        'b' : '0.75',\n",
    "        'k1' : 1.2\n",
    "    }\n",
    "}\n",
    "# we have to update using the actual index name not the alias\n",
    "# this is a known issue in Elasticsearch\n",
    "# you can check the index name with `se.index_info()` in the cell above\n",
    "se.update_ranking_model(f'{index_name}', sim_bm25f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371edb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "se.index_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign weights to fields\n",
    "fields = [\n",
    "    f'{field}^{str(weight)}' for field, weight in zip(\n",
    "        ['title', 'url', 'browser', 'last_visit_date'], \n",
    "        [1, 2, 3, 4]\n",
    "    )\n",
    "]\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f786f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# `combined_fields` was introduced in v7.13 and truly implements BM25F\n",
    "# as we running an old version (as provided by the lecturer), \n",
    "# we have to either use `multi_match` or bump the version (not sure we can)\n",
    "# see: https://opensourceconnections.com/blog/2021/06/30/better-term-centric-scoring-in-elasticsearch-with-bm25f-and-the-combined_fields-query/\n",
    "\n",
    "query = {\n",
    "    'query' : {\n",
    "        'multi_match' : {\n",
    "            'query' : 'sqlite',\n",
    "            'fields' : fields,\n",
    "            'type' : 'cross_fields'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "q1 = se.query(index_name, body = query)\n",
    "hits = se.get_hits(q1, *['url', 'title', 'visit_date', 'last_visit_date'], explain = False)\n",
    "print(json.dumps(hits, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19810ac5",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The test suite contains 3-tuples of the form (query id, document id, score). \n",
    "These are used to calculate the average effectiveness of the search engine by calculating\n",
    "* Recall and precision\n",
    "* F-score\n",
    "* Fall-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6df97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, relevance, res):\n",
    "        self.relevant_docs = [str(doc_id) for _, doc_id, score in relevance if score > 0]\n",
    "        self.retrieved_docs = [hit['_id'] for hit in res['hits']['hits']]\n",
    "        self.docs_intersection = set(self.relevant_docs).intersection(set(self.retrieved_docs))\n",
    "        \n",
    "    def recall(self):\n",
    "        try:\n",
    "            r = len(self.docs_intersection) / len(self.relevant_docs)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "        return r\n",
    "\n",
    "    def precision(self):\n",
    "        try:\n",
    "            p = len(self.docs_intersection) / len(self.retrieved_docs)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "        return p\n",
    "\n",
    "    def f_score(self):\n",
    "        try:\n",
    "            r = self.recall()\n",
    "            p = self.precision()\n",
    "            fs = 2 * (p * r) / (p + r)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "        return fs\n",
    "\n",
    "    def fall_out(self):\n",
    "        try:\n",
    "            fo = 1 - len(self.docs_intersection) / len(self.retrieved_docs)\n",
    "        except ZeroDivisionError:\n",
    "            return -1\n",
    "        return fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9167cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries:\n",
    "# 0. search for documents that have `sqlite ` in  title\n",
    "# 1. search for documents that have either history on the `title` or 'database' somewhere in the URL\n",
    "# 2. search for links from Chrome\n",
    "\n",
    "q_rj = [\n",
    "    [\n",
    "        (0, doc_id, score)\n",
    "        for doc_id, score in zip(\n",
    "            range(77),\n",
    "            [0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,1,1,2,2,2,2,1\n",
    "            ,2,0,0,0,0,0,0,0,2,2,2,2,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2\n",
    "            ,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]\n",
    "        )\n",
    "    ],\n",
    "    [\n",
    "        (1, doc_id, score)\n",
    "        for doc_id, score in zip(\n",
    "            range(77),\n",
    "            [0,0,0,0,2,2,1,1,2,2,1,2,2,1,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0\n",
    "            ,0,0,1,1,0,0,1,2,2,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,1,0,1,2\n",
    "            ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    )],\n",
    "    [\n",
    "        (2, doc_id, score)\n",
    "        for doc_id, score in zip(\n",
    "            range(77),\n",
    "            [0,2,2,0,2,2,2,2,0,2,2,0,2,2,0,2,0,2,0,2,2,2,2,0,2,0,2,0,2,2\n",
    "            ,0,0,2,2,2,0,0,2,0,2,2,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "            ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        )\n",
    "    ]\n",
    "]\n",
    "\n",
    "# q_rj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f66359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query hits for the queries above\n",
    "qs = [\n",
    "    se.query(index_name, body = {\n",
    "        'query' : {\n",
    "            'term'  : {\n",
    "                'title' : 'sqlite'\n",
    "            } \n",
    "        } \n",
    "    }),\n",
    "    se.query(index_name, body = {\n",
    "        'query' : {\n",
    "            'bool' : {\n",
    "                'should' : [\n",
    "                    {\n",
    "                        'match' : {\n",
    "                            'title' : 'history'\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'match' : {\n",
    "                            'url' : '/database/'\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }),\n",
    "    se.query(index_name, body = {\n",
    "        'query' : {\n",
    "            'bool' : {\n",
    "                'must' : [\n",
    "                    {\n",
    "                        'match' : {\n",
    "                            'visit_type' : 'link'\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'match' : {\n",
    "                            'browser' : 'chrome'\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "]\n",
    "for i, (rel, res) in enumerate(zip(q_rj, qs)):\n",
    "    # run the evaluator on the test queries\n",
    "    evaluator = Evaluator(rel, res)\n",
    "    print(f'q#{i}',\n",
    "          f'recall: {evaluator.recall():.3f}', \n",
    "          f'precision: {evaluator.precision():.3f}',\n",
    "          f'f-score: {evaluator.f_score():.3f}',\n",
    "          f'fall-out: {evaluator.fall_out():.3f}', \n",
    "          sep = \"\\n\", end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e3328",
   "metadata": {},
   "source": [
    "### User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1063afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c71b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_types = {\n",
    "    'match' : lambda field, query : {'query': {'match': {field: query}}},\n",
    "    'term' : lambda field, query : {'query': {'term': {field: query}}},\n",
    "    'range' : lambda field, query : {'query': {'range': {field: {'lte': query}}}},\n",
    "    'multi_match' :  lambda field, query : {'query': {'multi_match': {'query': query, 'type': 'cross_fields', 'fields': field}}}    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a01cdf71",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfc0937ff744c6b9ad0ba9533b5c37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Query type:', options=('match', 'term', 'range', 'multi_match'), value='match')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fb3e1258264e888070329789736963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Field:', placeholder='Enter field...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5528afd434841dfbe71bbc912ae108c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Weights:', placeholder='Enter weights...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351d3e53ddb14e979405ebd8dc6f2333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search:', placeholder='Enter search...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef989f0a3af7478c9c35e6416abff15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SearchInterface:\n",
    "    def __init__(self):\n",
    "        self.search_area = widgets.Text(\n",
    "            value = '',\n",
    "            placeholder = 'Enter search...',\n",
    "            description = 'Search:',\n",
    "        )\n",
    "\n",
    "        self.query_type_dd = widgets.Dropdown(\n",
    "            options = ['match', 'term', 'range', 'multi_match'],\n",
    "            value = 'match',\n",
    "            description = 'Query type:'\n",
    "        )\n",
    "\n",
    "        self.field_input = widgets.Text(\n",
    "            value = '',\n",
    "            options = ['url', 'title', 'last_visit_date'],\n",
    "            placeholder = 'Enter field...',\n",
    "            description = 'Field:',\n",
    "        )\n",
    "\n",
    "        self.weight_input = widgets.Text(\n",
    "            value = '',\n",
    "            placeholder = 'Enter weights...',\n",
    "            description = 'Weights:',\n",
    "        )\n",
    "\n",
    "        self.output = widgets.Output()\n",
    "\n",
    "    def search(self, change):\n",
    "        query_type = self.query_type_dd.value\n",
    "        field = self.field_input.value.strip()\n",
    "        weight = self.weight_input.value.strip()\n",
    "        query = self.search_area.value.strip()\n",
    "\n",
    "        query_f = query_types[query_type]\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            if query:\n",
    "                args = [field, query]\n",
    "                if query_type == 'multi_match':\n",
    "                    fields = [\n",
    "                        f'{field}^{weight}' \n",
    "                        for field, weight in zip(\n",
    "                            field.split(' '),\n",
    "                            weight.split(' ')\n",
    "                        )\n",
    "                    ]\n",
    "                    args[0] = fields                    \n",
    "                res = se.query(index_name, body = query_f(*args))\n",
    "                hits = se.get_hits(res, *['url', 'title', 'visit_date', 'last_visit_date'], explain = False, fmt = 'ascii')\n",
    "                print(hits)            \n",
    "    \n",
    "    def display(self):\n",
    "        self.search_area.observe(self.search, names='value')\n",
    "        # render widgets\n",
    "        display(self.query_type_dd, self.field_input, self.weight_input, self.search_area, self.output)\n",
    "\n",
    "            \n",
    "SearchInterface().display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
